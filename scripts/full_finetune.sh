python3 /data/ice/lt/llama-finetune/full_finetune.py \
--model_name_or_path "/vg_data/share/models/llama2-hf-converted/llama-2-7b" \
--device "cuda:1" \
--data_path /vg_data/share/dataset/alpaca/alpaca_data.json \
--bf16 True \
--output_dir "/vg_data/share/models/llama2-hf-converted/fine_tuning_result/alpaca" \
--per_device_train_batch_size 1 \
--per_device_eval_batch_size 1 \
--gradient_accumulation_steps 1 \
--evaluation_strategy "no" \
--save_strategy "steps" \
--save_steps 2000 \
--max_steps 50000 \
--save_total_limit 1 \
--optim "paged_adamw_8bit" \
--learning_rate 2e-5 \
--weight_decay 0. \
--warmup_ratio 0.03 \
--lr_scheduler_type "cosine" \
--logging_steps 1 \
--tf32 True \
--report_to none 